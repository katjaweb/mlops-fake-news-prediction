{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf6567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utilityFunctions' from '/workspaces/mlops-fake-news-prediction/utilityFunctions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils.utility_functions as utilityFunctions\n",
    "importlib.reload(utilityFunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527956f-04a3-4e75-af0e-2184d7fcafd8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c254004-0221-41c2-a81b-842ccea27eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import boto3\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0c530-f368-4918-9be3-bcb793828172",
   "metadata": {},
   "source": [
    "# Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8bb06a-1a3e-4261-85d1-6c5b19d68270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Function for data cleaning\n",
    "def clean_data(features, target):\n",
    "    # drop unused column\n",
    "    features = features.drop(columns='Unnamed: 0')\n",
    "    # Reverse labels into: fake=1, real=0\n",
    "    target = 1 - target\n",
    "    # Fill Nan-values with an empty string\n",
    "    features[['title', 'text']] = features[['title', 'text']].fillna(\"\")\n",
    "\n",
    "    return features, target\n",
    "\n",
    "# Function for calculation sentiment scores\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(str(text))\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Function for calculating the average word length\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# Function for calculating the number of sentences\n",
    "def sentence_count(text):\n",
    "    sentences = re.split(r'[.!?]', text)  # Sentence splitting for '.', '!', '?'\n",
    "    sentences = [s for s in sentences if s.strip()]  # Removes empty sentences\n",
    "    return len(sentences)\n",
    "\n",
    "# Function for counting special characters\n",
    "def special_char_count(text):\n",
    "    return len(re.findall(r'[^a-zA-Z0-9\\s]', text))  # Finds all special characters\n",
    "\n",
    "DetectorFactory.seed = 42  \n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def prepare_features(df):\n",
    "    df['title_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "    df[\"text_word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "    df[\"title_word_count\"] = df[\"title\"].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "    \n",
    "    df[\"text_unique_words\"] = df[\"text\"].apply(lambda x: len(set(x.lower().split())) if pd.notnull(x) else 0)\n",
    "    \n",
    "    df[\"text_char_count\"] = df[\"text\"].apply(lambda x: len(x) - x.count(' ') if pd.notnull(x) else 0)\n",
    "    df[\"title_char_count\"] = df[\"title\"].apply(lambda x: len(x) - x.count(' ') if pd.notnull(x) else 0)\n",
    "\n",
    "    # Applying the functions to the text column\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: avg_word_length(str(x)))\n",
    "    df['sentence_count'] = df['text'].apply(lambda x: sentence_count(str(x)))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: special_char_count(str(x)))\n",
    "    # df['language'] = df['text'].astype(str).apply(detect_language)\n",
    "    df['sentiment'] = df['text'].apply(get_sentiment)\n",
    "\n",
    "    df = df.drop(['title', 'text'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# `text_cleaner` function\n",
    "def text_cleaner(text):\n",
    "    \"\"\"Clean the text using NLP-Steps.\n",
    " \n",
    "    Steps include: Lemmatization, removing stop words, removing punctuations \n",
    " \n",
    "    Args:\n",
    "        sentence (str): The uncleaned text.\n",
    " \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Doc object named `text` from `sentence` using `nlp()`\n",
    "    doc = nlp(text)\n",
    "    # Lemmatization\n",
    "    lemma_token = [token.lemma_ for token in doc if token.pos_ != 'PRON']\n",
    "    # Remove stop words and converting tokens to lowercase\n",
    "    no_stopWords_lemma_token = [token.lower() for token in lemma_token if token not in stopWords]\n",
    "    # Remove punctuations\n",
    "    clean_doc = [token for token in no_stopWords_lemma_token if token not in punctuations]\n",
    "    # Use the `.join` method on `text` to convert string\n",
    "    joined_clean_doc = \" \".join(clean_doc)\n",
    "    # Use `re.sub()` to substitute multiple spaces or dots`[\\.\\s]+` to single space `' '\n",
    "    sub_doc = re.sub(r'[\\.\\s]+', ' ', joined_clean_doc)\n",
    "    # Use `re.sub()` to substitute â–  to an empty string `' '\n",
    "    final_doc = re.sub(r'[\\â– ðŸš¨]+', '', sub_doc)\n",
    "    \n",
    "    return final_doc\n",
    "\n",
    "\n",
    "def apply_text_cleaner(df, column):\n",
    "    # Progress bar\n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Progress\"):\n",
    "        results.append(text_cleaner(row[column]))\n",
    "\n",
    "    df['title_text_clean'] = results\n",
    "\n",
    "    df = df.drop(['title_text'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_file_s3(s3_bucket, dataset_path, type):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    if type == 'csv':\n",
    "        buffer = io.BytesIO()\n",
    "        s3.download_fileobj(s3_bucket, dataset_path, buffer)\n",
    "        buffer.seek(0)\n",
    "        data = pd.read_csv(buffer)\n",
    "        return data\n",
    "    else:\n",
    "        buffer = io.BytesIO()\n",
    "        s3.download_fileobj(s3_bucket, dataset_path, buffer)\n",
    "        buffer.seek(0)\n",
    "        data = pd.read_parquet(buffer)\n",
    "        return data\n",
    "\n",
    "def upload_to_s3(file, s3_bucket, dataset):\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    if isinstance(file, pd.Series):\n",
    "        buffer = io.BytesIO()\n",
    "        file.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "    else:\n",
    "        buffer = io.BytesIO()\n",
    "        file.to_parquet(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "\n",
    "    # aktuelles Datum im Format YYYY-MM-DD\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Beispiel-Dateiname mit Datum\n",
    "    file_key = f\"datasets/{dataset}_{date}.parquet\"\n",
    "    print(file_key)\n",
    "\n",
    "    s3_client.put_object(Bucket=s3_bucket, Key=file_key, Body=buffer.getvalue())\n",
    "    print(f\"File saved under {file_key} in {s3_bucket}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04ae0a-5967-456d-96c2-318d1cdc1d0e",
   "metadata": {},
   "source": [
    "# Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "611968d1-7a32-44aa-91a5-fbbb7dd58c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = os.getenv('S3_BUCKET', 'fake-news-prediction')    \n",
    "dataset_path = os.getenv(\"DATASET_PATH\", 'datasets/WELFake_Dataset.csv')\n",
    "\n",
    "df = load_file_s3(s3_bucket, dataset_path, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79794e8e-b8cc-4740-a839-b6ee7b9388b4",
   "metadata": {},
   "source": [
    "# Remove duplicates so that they do not end up in the validation and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe0ec92-b228-441e-9fcb-4038b7b129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='text').reset_index().drop(columns='index')\n",
    "df = df.drop_duplicates(subset='title').reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad14a6c6-145a-436a-9f27-c5d43a3c7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67525000-eed0-4051-811e-accb64f37d92",
   "metadata": {},
   "source": [
    "# Perform train-, val- and test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57519a28-38e6-48f4-873c-a258e397b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='label')\n",
    "y = df.loc[:, 'label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059d587-5329-436b-a349-0cf19bbe9a8d",
   "metadata": {},
   "source": [
    "# clean data and create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afe0d1-4521-43d0-8ef2-6010af796d64",
   "metadata": {},
   "source": [
    "clean data: drop unused column 'Unnamed: 0', reverse labels into: fake=1 and real=0, fill Nan-values with an empty string\n",
    "\n",
    "prepare features: Create new features\n",
    "- title_text: concatenate columns title and text\n",
    "- text_word_count: # of words in text column\n",
    "- title_word_count: # of words in title column\n",
    "- text_unique_words: # of uniqe words in text column\n",
    "- text_char_count: # of characters in text column\n",
    "- title_char_count: # of characters in title column\n",
    "- avg_word_length: average word length in text column\n",
    "- sentence_count: # of sentences in text column\n",
    "- special_char_count: # of special characters in text column\n",
    "- language: estimated language for text column\n",
    "- sentiment: calculate sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2053d596-b51a-4a5f-a754-dc227fb7dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)\n",
    "\n",
    "X_train = prepare_features(X_train)\n",
    "X_val = prepare_features(X_val)\n",
    "X_test = prepare_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825628d-d8b8-4989-b926-1d77074451c4",
   "metadata": {},
   "source": [
    "Apply text cleaner on column 'title_text': Using NLP-Steps include Lemmatization, removing stop words, removing punctuations and substitute multiple spaces or dots to single space. Returns new column 'title_text_clean' to DataFrame with the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5122d6c0-715e-4be6-a10b-a6d92785815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:54<00:00, 11.08it/s]\n",
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:17<00:00, 11.36it/s]\n",
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:16<00:00, 12.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = apply_text_cleaner(X_train, column='title_text')\n",
    "X_val = apply_text_cleaner(X_val, column='title_text')\n",
    "X_test = apply_text_cleaner(X_test, column='title_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f018bc",
   "metadata": {},
   "source": [
    "# Upload features and target to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d255644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/X_train_2025-04-07.parquet\n",
      "File saved under datasets/X_train_2025-04-07.parquet in fake-news-prediction.\n",
      "datasets/X_val_2025-04-07.parquet\n",
      "File saved under datasets/X_val_2025-04-07.parquet in fake-news-prediction.\n",
      "datasets/X_test_2025-04-07.parquet\n",
      "File saved under datasets/X_test_2025-04-07.parquet in fake-news-prediction.\n"
     ]
    }
   ],
   "source": [
    "upload_to_s3(X_train, s3_bucket, \"X_train\")\n",
    "upload_to_s3(X_val, s3_bucket, \"X_val\")\n",
    "upload_to_s3(X_test, s3_bucket, \"X_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9325f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/y_train_2025-04-07.parquet\n",
      "File saved under datasets/y_train_2025-04-07.parquet in fake-news-prediction.\n",
      "datasets/y_val_2025-04-07.parquet\n",
      "File saved under datasets/y_val_2025-04-07.parquet in fake-news-prediction.\n",
      "datasets/y_test_2025-04-07.parquet\n",
      "File saved under datasets/y_test_2025-04-07.parquet in fake-news-prediction.\n"
     ]
    }
   ],
   "source": [
    "upload_to_s3(y_train, s3_bucket, \"y_train\")\n",
    "upload_to_s3(y_val, s3_bucket, \"y_val\")\n",
    "upload_to_s3(y_test, s3_bucket, \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6943e3ad-c419-4660-a2ce-fc12b40e6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/X_train_2025-04-07.parquet\n"
     ]
    }
   ],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "# Beispiel-Dateiname mit Datum\n",
    "file_key = f\"datasets/X_train_{date}.parquet\"\n",
    "print(file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f654c6-0355-4b7e-b0c5-357bf2549a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example = load_file_s3(s3_bucket=s3_bucket, dataset_path=file_key, type='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9cf1739-75d1-41ac-aabf-0426c33845a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36840, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898056f-f092-4ed5-8471-7159192686c9",
   "metadata": {},
   "source": [
    "Use jupyter nbconvert --to script process_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77688c30-dcc2-404d-8259-c734f9d01276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
